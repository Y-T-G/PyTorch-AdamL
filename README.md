# PyTorch-AdamL
PyTorch implementation of the optimizer described in "AdamL: A fast adaptive gradient method incorporating loss function"
